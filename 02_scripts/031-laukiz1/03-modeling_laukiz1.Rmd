---
title: "03-modeling"
output: 
  bookdown::html_document2:
    toc: true
    toc_float: true
---

# Load packages

```{r, results='hide'}
library(mlr)
library(mlrMBO)
library(glue)
library(sf)
library(dplyr)
library(tibble)
library(purrr)
library(magrittr)
library(data.table)
library(pbmcapply)
library(stringr)
library(ggpubr)
```

# Read data

Variables per plot:

Lauki1: 7594
Laukiz2: 7594
Oiartzun: 7891
Luiando: 6073

Why is there such a huge difference in the number of indices without NAs for the plots?

```{r}
data = readRDS("/data/patrick/mod/hyperspectral/data_clean_with_indices/data_clean_standardized_bf2.rda")
coords = readRDS("/data/patrick/mod/hyperspectral/data_clean_with_indices/data_clean_standardized_bf2-coords.rda")
```

# Select plot

```{r}
data = data[[1]]
coords = coords[[1]]
```

Subset to indices shared by all plots.

```{r}
indices_shared = readRDS("/data/patrick/mod/hyperspectral/data_clean_with_indices/indices-shared-by-all-plots-bf2.rda")

data %<>% 
  dplyr::select(indices_shared)
```

# Create task

```{r}
task = makeRegrTask(id = "laukiz1", data = data, 
                    target = "defoliation", coordinates = coords)
```

# Modeling

## glmnet

### Ridge

We only need to tune `s`: https://github.com/mlr-org/mlr/issues/1030#issuecomment-233677172
`glmnet` fits many models during training. However, important is which `s` is chosen for prediction. 
The value of `s` determines which trained model will be chosen.
If `s` != `lambda`, linear interpolation or refitting (this) is applied.

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda` values.

Besides tuning with `mlr`, we could use `cv-glmnet()` to find a suitable `lambda` value for a given dataset.
However, we would do a random partitioning instead of a spatial partitioning here.

Coefficients of ridge regression should not be used for inference as they are biased. 
They can be seen as a variable importance measure. 
The highest coefficients are most important for the model (if all variables are on the same scale) https://stats.stackexchange.com/a/154712/101464

```{r}
# predict.type for 'auc' (AUROC)
lrn_glmnet <- makeLearner("regr.glmnet",
                          alpha = 0,
                          standardize = FALSE,
                          intercept = FALSE)
                          #lambda = sort(seq(0, 5, length.out = 100), decreasing = T))
# getHyperPars(lrn_glmnet)
# getParamSet(lrn_glmnet)
```

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda`.

```{r}
glmnet_train = mlr::train(lrn_glmnet, task)
summary(glmnet_train$learner.model$lambda)
```

```{r}
ps_glmnet <- makeParamSet(makeNumericParam("s", lower = 78, upper = 7879))
```

```{r}
ctrl = makeMBOControl(propose.points = 1L)
ctrl = setMBOControlTermination(ctrl, iters = 70L)
ctrl = setMBOControlInfill(ctrl, crit = crit.ei)
ctrl = setMBOControlMultiPoint(ctrl, method = "cl", cl.lie = min)
tune.ctrl = makeTuneControlMBO(mbo.control = ctrl,
                               mbo.design = generateDesign(n = 30, par.set =  ps_glmnet))
inner <- makeResampleDesc("SpCV", iters = 10)
```

#### SpCV

```{r}
tune_wrapper <- makeTuneWrapper(lrn_glmnet, resampling = inner, par.set = ps_glmnet,
                                control = tune.ctrl, show.info = TRUE,
                                measures = list(rmse))
```

```{r}
outer <- makeResampleDesc("SpRepCV", folds = 10, reps = 20)
```

```{r}
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.resample", cpus = 20,
              mc.set.seed = TRUE) # only parallelize the tuning

set.seed(12345)
resa_glmnet <- mlr::resample(tune_wrapper, task,
                             resampling = outer, extract = getTuneResult,
                             show.info = TRUE, measures = list(rmse))
parallelStop()
saveRDS(resa_glmnet, "/data/patrick/mod/hyperspectral/spcv/laukiz1_ridge_bf2_10folds.rda")
resa_glmnet = readRDS("/data/patrick/mod/hyperspectral/spcv/laukiz1_ridge_bf2_10folds.rda")
```

Calculate repetition mean

```{r}
resa_glmnet = readRDS("/data/patrick/mod/hyperspectral/spcv/laukiz1_ridge_bf2_10folds.rda")
resa_glmnet$measures.test %>% 
  dplyr::mutate(rep = rep(1:20, times = 1, each = 10)) %>% 
  group_by(rep) %>% 
  summarise_at("rmse", mean) %>% 
  summarise(mean(rmse), sd(rmse))
```


#### TuneParams

```{r}
configureMlr(on.learner.error = "warn", on.error.dump = TRUE)
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.tuneParams", cpus = 48,
              mc.set.seed = TRUE) # only parallelize the tuning
set.seed(12345)
params_tuned_glmnet = tuneParams(lrn_glmnet, task = task, resampling = inner,
                                 par.set = ps_glmnet, control = tune.ctrl, 
                                 measure = list(rmse))
parallelStop()
saveRDS(params_tuned_glmnet, "/data/patrick/mod/tmp/tuning_hyperspectral/tuning_bf2_ridge.rda")
```

#### Tuned results

We use the `s` value from the tuning. 
As we will always use the complete dataset, this should be the best value.

```{r}
lrn_glmnet_tuned <- makeLearner("regr.glmnet",
                                alpha = 0,
                                s = 7500,
                                standardize = FALSE, 
                                intercept = FALSE)

glmnet_train_tuned = mlr::train(lrn_glmnet_tuned, task)
```

Inspect the highest absolute coefficients (positive and negative).
The first entry is the intercept and is therefore left out.

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 7500))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 7500)))) %>% 
  arrange(desc(coef))
```

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 7500))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 7500)))) %>% 
  arrange(coef)
```

```{r}
plot(glmnet_train_tuned$learner.model, "lambda")
```

```{r}
plot(coef(glmnet_train_tuned$learner.model))
```

#### Important spectral regions

Get the first 100 most important indices.
We take the absolute value of the coefficient to consider both sides.

We only take the top 100 indices so that the result is not cluttered by mid-ranged indices summping up in the end-result.
Apparently, we have 56 unique bands within the top 100 indices.

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 7500))), 
       coef = abs(as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 7500))))) %>% 
  arrange(desc(coef)) %>% 
  mutate(coef = coef) -> top_inds_rr_laukiz1
saveRDS(top_inds_rr_laukiz1, "/data/patrick/mod/hyperspectral/top-inds/top_inds_rr_laukiz1.rda")
```

Remove the "bf2" information and filter out NRI only.

```{r}
top_inds_rr_laukiz1 %<>% 
  mutate(index = str_sub(index, 9, 15)) %>% 
  filter(str_detect(index, "b"))
```

Standardize the coefficients

```{r}
top_inds_rr_laukiz1 %<>% 
  mutate(coef = as.numeric((coef / min(coef)) * coef))
```

Extract all band numbers. 

```{r}
top_inds_rr_laukiz1 %>% 
  dplyr::select(index) %>% 
  str_match_all("[0-9]+") %>% 
  flatten() %>% 
  as.numeric() -> all_bands_rr
```

```{r}
hist(all_bands_rr)
```

Now add a weight to all bands.
Starting from 100-2 depending on the score.
why 2? Because we only have 198 values because we removed one vegetation index.

```{r}
tibble(band = all_bands_rr, weight = rep(top_inds_rr_laukiz1$coef, times = 1, each = 2)) %>% 
  group_by(band) %>% 
  summarise(score = sum(weight)) %>% 
  mutate(band = band) %>% 
  arrange(desc(score)) -> tibble_most_imp
```

Classify the bands into spectral regions

1 - 60: visible (404 - 680)
61 - 71: red edge (680 - 730)
72 - 126: NIR (730 - 996)


```{r}
tibble_most_imp %<>% 
  mutate(group = as.factor(case_when(band <= 60 ~ "Visible (404 - 680)", 
                           (band >= 61 & band <= 71) ~ "Red Edge (680 - 730)",
                           band >= 72 ~ "NIR (730 - 996)"))) %>% 
  arrange(desc(score)) %>% 
  mutate(band = as.factor(band))
```

centered by the mean of the scaled coefficients.

```{r}
ggdotchart(tibble_most_imp, x = "band", y = "score",                              # Color by groups
           sorting = "descending",                       # Sort value in descending order
           add = "segments",                             # Add segments from y = 0 to dots
           color = "group",
           palette = "nejm",
           group = "score",
           combine = T,
           rotate = TRUE,                                # Rotate vertically
           dot.size = 1,                                 # Large dot size
           #label = round(tibble_most_imp$sum),                        # Add mpg values as dot labels
           ggtheme = theme_pubr(base_size = 9, margin = FALSE)) + 
  labs(y = "Score", color = "Spectral regions")
```


### Lasso

We only need to tune `s`: https://github.com/mlr-org/mlr/issues/1030#issuecomment-233677172
`glmnet` fits many models during training. However, important is which `s` is chosen for prediction. 
The value of `s` determines which trained model will be chosen.
If `s` != `lambda`, linear interpolation or refitting is applied.

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda` values.

Besides tuning with `mlr`, we could use `cv-glmnet()` to find a suitable `lambda` value for a given dataset.
However, we would do a random partitioning instead of a spatial partitioning here.

Coefficients of ridge regression should not be used for inference as they are biased. 
They can be seen as a variable importance measure then? The highest coefficients are most important for the model (if all variables are on the same scale) https://stats.stackexchange.com/a/154712/101464

```{r}
lrn_glmnet <- makeLearner("regr.glmnet",
                          alpha = 1,
                          standardize = FALSE,
                          intercept = FALSE)
# getHyperPars(lrn_glmnet)
# getParamSet(lrn_glmnet)
```

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda`.

```{r}
glmnet_train = mlr::train(lrn_glmnet, task)
summary(glmnet_train$learner.model$lambda)
```

```{r}
ps_glmnet <- makeParamSet(makeNumericParam("s", lower = 0.07, upper = 7.87))
```

```{r}
ctrl = makeMBOControl(propose.points = 1L)
ctrl = setMBOControlTermination(ctrl, iters = 70L)
ctrl = setMBOControlInfill(ctrl, crit = crit.ei)
ctrl = setMBOControlMultiPoint(ctrl, method = "cl", cl.lie = min)
tune.ctrl = makeTuneControlMBO(mbo.control = ctrl,
                               mbo.design = generateDesign(n = 30, par.set =  ps_glmnet))
inner <- makeResampleDesc("SpCV", iters = 10)
```

#### SpCV

```{r}
tune_wrapper <- makeTuneWrapper(lrn_glmnet, resampling = inner, par.set = ps_glmnet,
                                control = tune.ctrl, show.info = TRUE,
                                measures = list(rmse))
```

```{r}
outer <- makeResampleDesc("SpRepCV", folds = 10, reps = 20)
```

```{r}
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.resample", cpus = 10,
              mc.set.seed = TRUE) # only parallelize the tuning

set.seed(12345)
resa_glmnet <- mlr::resample(tune_wrapper, task,
                             resampling = outer, extract = getTuneResult,
                             show.info = TRUE, measures = list(rmse))

parallelStop()
saveRDS(resa_glmnet, "/data/patrick/mod/hyperspectral/spcv/laukiz1_lasso_bf2_10folds.rda")
```

Calculate repetition mean

```{r}
resa_glmnet = readRDS("/data/patrick/mod/hyperspectral/spcv/laukiz1_lasso_bf2_10folds.rda")
resa_glmnet$measures.test %>% 
  dplyr::mutate(rep = rep(1:20, times = 1, each = 10)) %>% 
  group_by(rep) %>% 
  summarise_at("rmse", mean) %>% 
  summarise(mean(rmse), sd(rmse))
```

#### TuneParams

```{r}
configureMlr(on.learner.error = "warn", on.error.dump = TRUE)
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.tuneParams", cpus = 40,
              mc.set.seed = TRUE) # only parallelize the tuning
set.seed(12345)
params_tuned_glmnet = tuneParams(lrn_glmnet, task = task, resampling = inner,
                                 par.set = ps_glmnet, control = tune.ctrl, 
                                 measure = list(rmse))
parallelStop()
```

#### Tuned results

We use the `s` value from the tuning. 
As we will always use the complete dataset, this should be the best value.

```{r}
lrn_glmnet_tuned <- makeLearner("regr.glmnet",
                                alpha = 1,
                                s = 1810,
                                standardize = FALSE, 
                                intercept = FALSE)
#lambda = sort(seq(0, 5, length.out = 100), decreasing = T))
glmnet_train_tuned = mlr::train(lrn_glmnet_tuned, task)
```

Inspect the highest absolute coefficients (positive and negative).
The first entry is the intercept and is therefore left out.

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810)))) %>% 
  arrange(desc(coef))
```

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810)))) %>% 
  arrange(coef)
```

```{r}
plot(glmnet_train_tuned$learner.model, "norm")
```

```{r}
plot(coef(glmnet_train_tuned$learner.model))
```

### Elasticnet

We only need to tune `s`: https://github.com/mlr-org/mlr/issues/1030#issuecomment-233677172
`glmnet` fits many models during training. However, important is which `s` is chosen for prediction. 
The value of `s` determines which trained model will be chosen.
If `s` != `lambda`, linear interpolation or refitting is applied.

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda` values.

Besides tuning with `mlr`, we could use `cv-glmnet()` to find a suitable `lambda` value for a given dataset.
However, we would do a random partitioning instead of a spatial partitioning here.

Coefficients of ridge regression should not be used for inference as they are biased. 
They can be seen as a variable importance measure then? The highest coefficients are most important for the model (if all variables are on the same scale) https://stats.stackexchange.com/a/154712/101464

```{r}
lrn_glmnet <- makeLearner("regr.glmnet",
                          standardize = FALSE,
                          intercept = FALSE)
#getHyperPars(lrn_glmnet)
#getParamSet(lrn_glmnet)
```

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda`.

```{r}
glmnet_train = mlr::train(lrn_glmnet, task)
summary(glmnet_train$learner.model$lambda)
```

For the elasticnet we also tune `alpha`.
As we have both penalties, we just use the lower and uppper maxima from both Lasso and Ridge.

```{r}
ps_glmnet <- makeParamSet(makeNumericParam("s", lower = 0.07, upper = 7879),
                          makeNumericParam("alpha", lower = 0, upper = 1))
```

```{r}
ctrl = makeMBOControl(propose.points = 1L)
ctrl = setMBOControlTermination(ctrl, iters = 70L)
ctrl = setMBOControlInfill(ctrl, crit = crit.ei)
ctrl = setMBOControlMultiPoint(ctrl, method = "cl", cl.lie = min)
tune.ctrl = makeTuneControlMBO(mbo.control = ctrl,
                               mbo.design = generateDesign(n = 30, par.set =  ps_glmnet))
inner <- makeResampleDesc("SpCV", iters = 10)
```

#### SpCV

```{r}
tune_wrapper <- makeTuneWrapper(lrn_glmnet, resampling = inner, par.set = ps_glmnet,
                                control = tune.ctrl, show.info = TRUE,
                                measures = list(rmse))
```

```{r}
outer <- makeResampleDesc("SpRepCV", folds = 10, reps = 20)
```

```{r}
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.resample", cpus = 10,
              mc.set.seed = TRUE) # only parallelize the tuning

set.seed(12345)
resa_glmnet <- mlr::resample(tune_wrapper, task,
                             resampling = outer, extract = getTuneResult,
                             show.info = TRUE, measures = list(rmse))

parallelStop()
saveRDS(resa_glmnet, "/data/patrick/mod/hyperspectral/spcv/laukiz1_elasticnet_bf2_10folds.rda")
```

Calculate repetition mean

```{r}
resa_glmnet = readRDS("/data/patrick/mod/hyperspectral/spcv/laukiz1_elasticnet_bf2_10folds.rda")
resa_glmnet$measures.test %>% 
  dplyr::mutate(rep = rep(1:20, times = 1, each = 10)) %>% 
  group_by(rep) %>% 
  summarise_at("rmse", mean) %>% 
  summarise(mean(rmse), sd(rmse))
```

#### TuneParams

```{r}
configureMlr(on.learner.error = "warn", on.error.dump = TRUE)
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.tuneParams", cpus = 48,
              mc.set.seed = TRUE) # only parallelize the tuning
set.seed(12345)
params_tuned_glmnet = tuneParams(lrn_glmnet, task = task, resampling = inner,
                                 par.set = ps_glmnet, control = tune.ctrl, 
                                 measure = list(rmse))
parallelStop()
```

#### Tuned results

We use the `s` value from the tuning. 
As we will always use the complete dataset, this should be the best value.

```{r}
lrn_glmnet_tuned <- makeLearner("regr.glmnet",
                                alpha = 0,
                                s = 1810,
                                standardize = FALSE, 
                                intercept = FALSE)
#lambda = sort(seq(0, 5, length.out = 100), decreasing = T))
glmnet_train_tuned = mlr::train(lrn_glmnet_tuned, task)
```

Inspect the highest absolute coefficients (positive and negative).
The first entry is the intercept and is therefore left out.

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810)))) %>% 
  arrange(desc(coef))
```

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810)))) %>% 
  arrange(coef)
```

```{r}
plot(glmnet_train_tuned$learner.model, "lambda")
```

```{r}
plot(coef(glmnet_train_tuned$learner.model))
```
