---
title: "03-modeling"
output: 
  bookdown::html_document2:
    toc: true
    toc_float: true
---

```{r, results='hide'}
# devtools::install_github("tidyverse/ggplot2")
devtools::install_github("mlr-org/mlr@blocking")

library(mlr)
library(mlrMBO)
library(glue)
library(sf)
library(dplyr)
library(tibble)
library(purrr)
library(magrittr)
library(data.table)
library(pbmcapply)
```

# Read data

Variables per plot:

Lauki1: 7594
Laukiz2: 7594
Oiartzun: 7841
Luiando: 7471

Why is there such a huge difference in the number of indices without NAs for the plots?

```{r}
data <- readRDS("/data/patrick/mod/hyperspectral/data_clean_with_indices/data_clean_standardized_bf2.rda")
coords <- readRDS("/data/patrick/mod/hyperspectral/data_clean_with_indices/data_clean_standardized_bf2-coords.rda")
```

# Merge into one dataset

```{r}
data <- as_tibble(rbindlist(data, fill = TRUE))
data <- Filter(function(x) !any(is.na(x)), data)
coords <- as_tibble(rbindlist(coords))
```

Save the indices shared by all plots so that we can subset the single plots to use the same indices

```{r}
indices_shared <- names(data)
saveRDS(indices_shared, "/data/patrick/mod/hyperspectral/data_clean_with_indices/indices-shared-by-all-plots-bf2.rda")
```

# Create task

```{r}
task <- makeRegrTask(
  id = "all_plots", data = data,
  target = "defoliation", coordinates = coords,
  blocking = factor(rep(1:4, c(479, 451, 291, 529)))
)
```

# Benchmark

## Tune strategy

```{r}
ctrl <- makeMBOControl(propose.points = 1L)
ctrl <- setMBOControlTermination(ctrl, iters = 70L)
ctrl <- setMBOControlInfill(ctrl, crit = crit.ei)
ctrl <- setMBOControlMultiPoint(ctrl, method = "cl", cl.lie = min)

inner <- makeResampleDesc("Blocking")
```

## Run

```{r}
# SVM
ps_svm <- makeParamSet(
  makeNumericParam("C",
                   lower = -15, upper = 15,
                   trafo = function(x) 2^x
  ),
  makeNumericParam("sigma",
                   lower = -15, upper = 15,
                   trafo = function(x) 2^x
  )
)
tune.ctrl <- makeTuneControlMBO(
  mbo.control = ctrl,
  mbo.design = generateDesign(n = 30, par.set = ps_svm)
)
tune_wrapper_svm <- makeTuneWrapper(makeLearner("regr.ksvm", kernel = "rbfdot"),
                                    resampling = inner, par.set = ps_svm,
                                    control = tune.ctrl, show.info = TRUE,
                                    measures = list(rmse)
)

# RIDGE
ps_glmnet <- makeParamSet(makeNumericParam("s", lower = 35, upper = 3559))
tune.ctrl <- makeTuneControlMBO(
  mbo.control = ctrl,
  mbo.design = generateDesign(n = 30, par.set = ps_glmnet)
)
tune_wrapper_glmnet <- makeTuneWrapper(makeLearner("regr.glmnet",
                                                   alpha = 0,
                                                   standardize = FALSE,
                                                   intercept = FALSE),
                                       resampling = inner, par.set = ps_glmnet,
                                       control = tune.ctrl, show.info = TRUE,
                                       measures = list(rmse)
)

# XGBOOST
ps_xgboost <- makeParamSet(
  makeNumericParam("nrounds", lower = 0, upper = 8.64, 
                   trafo = function (x) round(2^x * 10)),
  makeNumericParam("colsample_bytree", lower = 0.3, upper = 0.7),
  makeNumericParam("subsample", lower = 0.25, upper = 1),
  makeIntegerParam("max_depth", lower = 1, upper = 10),
  makeNumericParam("gamma", lower = 0, upper = 10),
  makeNumericParam("eta", lower = 0.001, upper = 0.6),
  makeNumericParam("min_child_weight", lower = 0, upper = 20)
)
tune.ctrl <- makeTuneControlMBO(
  mbo.control = ctrl,
  mbo.design = generateDesign(n = 30, par.set = ps_xgboost)
)
tune_wrapper_xgboost <- makeTuneWrapper(makeLearner("regr.xgboost",
                                                    par.vals = list(
                                                      objective = "reg:linear",
                                                      eval_metric = "error"
                                                    )
),
resampling = inner, par.set = ps_xgboost,
control = tune.ctrl, show.info = TRUE,
measures = list(rmse)
)

# RF
ps_rf <- makeParamSet(
  makeIntegerParam("mtry", lower = 1, upper = 7000),
  makeIntegerParam("min.node.size", lower = 1, upper = 10),
  makeNumericParam("sample.fraction", lower = 0.2, upper = 0.9)
)
tune.ctrl <- makeTuneControlMBO(
  mbo.control = ctrl,
  mbo.design = generateDesign(n = 30, par.set = ps_rf)
)
tune_wrapper_rf <- makeTuneWrapper(makeLearner("regr.ranger"),
                                   resampling = inner, par.set = ps_rf,
                                   control = tune.ctrl, show.info = TRUE,
                                   measures = list(rmse)
)

lrns <- list(
  tune_wrapper_svm, tune_wrapper_glmnet,
  tune_wrapper_xgboost, tune_wrapper_rf
)

outer <- makeResampleDesc("Blocking")

library(parallelMap)
parallelStart(
  mode = "multicore", level = "mlrMBO.feval", cpus = 30,
  mc.set.seed = TRUE
) # only parallelize the tuning

set.seed(12345)
bmr <- benchmark(lrns, task, outer, measures = rmse, show.info = TRUE)
parallelStop()
saveRDS(bmr, "/data/patrick/mod/hyperspectral/spcv/all_plots_benchmark.rda")
```

