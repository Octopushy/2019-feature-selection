---
title: "03-modeling"
output: 
  bookdown::html_document2:
    toc: true
    toc_float: true
---

# Load packages

```{r, results='hide'}
devtools::install_github("tidyverse/ggplot2")
devtools::install_github("mlr-org/mlr@blocking")

library(mlr)
library(glue)
library(sf)
library(dplyr)
library(tibble)
library(purrr)
library(magrittr)
library(data.table)
library(pbmcapply)
```

# Read data

Variables per plot:

Lauki1: 7594
Laukiz2: 7594
Oiartzun: 7841
Luiando: 7471

Why is there such a huge difference in the number of indices without NAs for the plots?

```{r}
data = readRDS("/data/patrick/mod/hyperspectral/data_clean_with_indices/data_clean_standardized_bf2.rda")
coords = readRDS("/data/patrick/mod/hyperspectral/data_clean_with_indices/data_clean_standardized_bf2-coords.rda")
```

# Merge into one dataset

Leaving out Luiando (see README for reason).

```{r}
data = as_tibble(rbindlist(data, fill = TRUE))
data = Filter(function(x) !any(is.na(x)), data)
coords = as_tibble(rbindlist(coords))
```

Save the indices shared by all plots so that we can subset the single plots to use the same indices

```{r}
indices_shared = names(data)
saveRDS(indices_shared, "/data/patrick/mod/hyperspectral/data_clean_with_indices/indices-shared-by-all-plots-bf2.rda")
```

# Create task

```{r}
task = makeRegrTask(id = "all_plots", data = data, 
                    target = "deftot", coordinates = coords,
                    blocking = factor(rep(1:4, c(559, 382, 301, 499))))
```

# Modeling

## glmnet

### Ridge

We only need to tune `s`: https://github.com/mlr-org/mlr/issues/1030#issuecomment-233677172
`glmnet` fits many models during training. However, important is which `s` is chosen for prediction. 
The value of `s` determines which trained model will be chosen.
If `s` != `lambda`, linear interpolation or refitting (this) is applied.

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda` values.

Besides tuning with `mlr`, we could use `cv-glmnet()` to find a suitable `lambda` value for a given dataset.
However, we would do a random partitioning instead of a spatial partitioning here.

Coefficients of ridge regression should not be used for inference as they are biased. 
They can be seen as a variable importance measure. 
The highest coefficients are most important for the model (if all variables are on the same scale) https://stats.stackexchange.com/a/154712/101464

```{r}
# predict.type for 'auc' (AUROC)
lrn_glmnet <- makeLearner("regr.glmnet",
                          alpha = 0,
                          standardize = FALSE,
                          intercept = FALSE)
                          #lambda = sort(seq(0, 5, length.out = 100), decreasing = T))
# getHyperPars(lrn_glmnet)
# getParamSet(lrn_glmnet)
```

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda`.

```{r}
glmnet_train = mlr::train(lrn_glmnet, task)
summary(glmnet_train$learner.model$lambda)
```

```{r}
ps_glmnet <- makeParamSet(makeNumericParam("s", lower = 35, upper = 3559))
```

```{r}
ctrl <- makeTuneControlRandom(maxit = 200)
inner <- makeResampleDesc("Blocking")
```

#### SpCV

```{r}
tune_wrapper <- makeTuneWrapper(lrn_glmnet, resampling = inner, par.set = ps_glmnet,
                                control = ctrl, show.info = TRUE,
                                measures = list(rmse))
```

```{r}
# outer <- makeResampleDesc("RepCV", folds = 4, reps = 2)
outer <- makeResampleDesc("Blocking")
```

```{r}
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.tuneParams", cpus = 40,
              mc.set.seed = TRUE) # only parallelize the tuning

set.seed(12345)
resa_glmnet <- mlr::resample(tune_wrapper, task,
                             resampling = outer, extract = getTuneResult,
                             show.info = TRUE, measures = list(rmse))
parallelStop()
saveRDS(resa_glmnet, "/data/patrick/mod/hyperspectral/spcv/all_plots_ridge_bf2_10folds_blocking.rda")
resa_glmnet = readRDS("/data/patrick/mod/hyperspectral/spcv/all_plots_ridge_bf2_10folds_blocking.rda")
```

#### Visualize partitions

```{r}
resa_glmnet= readRDS("/data/patrick/mod/hyperspectral/spcv/all_plots_ridge_bf2_10folds.rda")
plot = createSpatialResamplingPlots(task, list("Blocking" = resa_glmnet), crs = 32630)
cowplot::plot_grid(plotlist = plot[["Plots"]], ncol = 4, nrow = 1, labels = plot[["Labels"]])
```

#### TuneParams

```{r}
configureMlr(on.learner.error = "warn", on.error.dump = TRUE)
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.tuneParams", cpus = 48,
              mc.set.seed = TRUE) # only parallelize the tuning
set.seed(12345)
params_tuned_glmnet = tuneParams(lrn_glmnet, task = task, resampling = inner,
                                 par.set = ps_glmnet, control = ctrl, 
                                 show.info = TRUE, measure = list(rmse))
parallelStop()
saveRDS(params_tuned_glmnet, "/data/patrick/mod/tmp/tuning_hyperspectral/tuning_bf2_ridge_blocking.rda")
```

#### Tuned results

We use the `s` value from the tuning. 
As we will always use the complete dataset, this should be the best value.

```{r}
lrn_glmnet_tuned <- makeLearner("regr.glmnet",
                                alpha = 0,
                                s = 875,
                                standardize = FALSE, 
                                intercept = FALSE)

glmnet_train_tuned = mlr::train(lrn_glmnet_tuned, task)
```

Inspect the highest absolute coefficients (positive and negative).
The first entry is the intercept and is therefore left out.

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 875))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 875)))) %>% 
  arrange(desc(coef))
```

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 875))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 875)))) %>% 
  arrange(coef)
```

```{r}
plot(glmnet_train_tuned$learner.model, "lambda")
```

```{r}
plot(coef(glmnet_train_tuned$learner.model))
```

### Lasso

We only need to tune `s`: https://github.com/mlr-org/mlr/issues/1030#issuecomment-233677172
`glmnet` fits many models during training. However, important is which `s` is chosen for prediction. 
The value of `s` determines which trained model will be chosen.
If `s` != `lambda`, linear interpolation or refitting is applied.

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda` values.

Besides tuning with `mlr`, we could use `cv-glmnet()` to find a suitable `lambda` value for a given dataset.
However, we would do a random partitioning instead of a spatial partitioning here.

Coefficients of ridge regression should not be used for inference as they are biased. 
They can be seen as a variable importance measure then? The highest coefficients are most important for the model (if all variables are on the same scale) https://stats.stackexchange.com/a/154712/101464

```{r}
lrn_glmnet <- makeLearner("regr.glmnet",
                          alpha = 1,
                          standardize = FALSE,
                          intercept = FALSE)
# getHyperPars(lrn_glmnet)
# getParamSet(lrn_glmnet)
```

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda`.

```{r}
glmnet_train = mlr::train(lrn_glmnet, task)
summary(glmnet_train$learner.model$lambda)
```

```{r}
ps_glmnet <- makeParamSet(makeNumericParam("s", lower = 0.03, upper = 3.5))
```

```{r}
ctrl <- makeTuneControlRandom(maxit = 200)
inner <- makeResampleDesc("SpCV", iters = 10)
inner <- makeResampleDesc("Blocking")
```

#### SpCV

```{r}
tune_wrapper <- makeTuneWrapper(lrn_glmnet, resampling = inner, par.set = ps_glmnet,
                                control = ctrl, show.info = TRUE,
                                measures = list(rmse))
```

```{r}
outer <- makeResampleDesc("Blocking")
```

```{r}
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.tuneParams", cpus = 48,
              mc.set.seed = TRUE) # only parallelize the tuning

set.seed(12345)
resa_glmnet <- mlr::resample(tune_wrapper, task,
                             resampling = outer, extract = getTuneResult,
                             show.info = TRUE, measures = list(rmse))

parallelStop()
saveRDS(resa_glmnet, "/data/patrick/mod/hyperspectral/spcv/all_plots_lasso_bf2_10folds_blocking.rda")
```

#### TuneParams

```{r}
configureMlr(on.learner.error = "warn", on.error.dump = TRUE)
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.tuneParams", cpus = 40,
              mc.set.seed = TRUE) # only parallelize the tuning
set.seed(12345)
params_tuned_glmnet = tuneParams(lrn_glmnet, task = task, resampling = inner,
                                 par.set = ps_glmnet, control = ctrl, 
                                 measure = list(rmse))
parallelStop()
```

#### Tuned results

We use the `s` value from the tuning. 
As we will always use the complete dataset, this should be the best value.

```{r}
lrn_glmnet_tuned <- makeLearner("regr.glmnet",
                                alpha = 0,
                                s = 1810,
                                standardize = FALSE, 
                                intercept = FALSE)
#lambda = sort(seq(0, 5, length.out = 100), decreasing = T))
glmnet_train_tuned = mlr::train(lrn_glmnet_tuned, task)
```

Inspect the highest absolute coefficients (positive and negative).
The first entry is the intercept and is therefore left out.

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810)))) %>% 
  arrange(desc(coef))
```

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810)))) %>% 
  arrange(coef)
```

```{r}
plot(glmnet_train_tuned$learner.model, "norm")
```

```{r}
plot(coef(glmnet_train_tuned$learner.model))
```

### Elasticnet

We only need to tune `s`: https://github.com/mlr-org/mlr/issues/1030#issuecomment-233677172
`glmnet` fits many models during training. However, important is which `s` is chosen for prediction. 
The value of `s` determines which trained model will be chosen.
If `s` != `lambda`, linear interpolation or refitting is applied.

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda` values.

Besides tuning with `mlr`, we could use `cv-glmnet()` to find a suitable `lambda` value for a given dataset.
However, we would do a random partitioning instead of a spatial partitioning here.

Coefficients of ridge regression should not be used for inference as they are biased. 
They can be seen as a variable importance measure then? The highest coefficients are most important for the model (if all variables are on the same scale) https://stats.stackexchange.com/a/154712/101464

```{r}
lrn_glmnet <- makeLearner("regr.glmnet",
                          standardize = FALSE,
                          intercept = FALSE)
#getHyperPars(lrn_glmnet)
#getParamSet(lrn_glmnet)
```

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda`.

```{r}
glmnet_train = mlr::train(lrn_glmnet, task)
summary(glmnet_train$learner.model$lambda)
```

For the elasticnet we also tune `alpha`.
As we have both penalties, we just use the lower and uppper maxima from both Lasso and Ridge.

```{r}
ps_glmnet <- makeParamSet(makeNumericParam("s", lower = 0.03, upper = 3559),
                          makeNumericParam("alpha", lower = 0, upper = 1))
```

```{r}
ctrl <- makeTuneControlRandom(maxit = 200)
inner <- makeResampleDesc("Blocking")
```

#### SpCV

```{r}
tune_wrapper <- makeTuneWrapper(lrn_glmnet, resampling = inner, par.set = ps_glmnet,
                                control = ctrl, show.info = TRUE,
                                measures = list(rmse))
```

```{r}
outer <- makeResampleDesc("Blocking")
```

```{r}
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.tuneParams", cpus = 40,
              mc.set.seed = TRUE) # only parallelize the tuning

set.seed(12345)
resa_glmnet <- mlr::resample(tune_wrapper, task,
                             resampling = outer, extract = getTuneResult,
                             show.info = TRUE, measures = list(rmse))

parallelStop()
saveRDS(resa_glmnet, "/data/patrick/mod/hyperspectral/spcv/all_plots_elasticnet_blocking.rda")
```

#### TuneParams

```{r}
configureMlr(on.learner.error = "warn", on.error.dump = TRUE)
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.tuneParams", cpus = 48,
              mc.set.seed = TRUE) # only parallelize the tuning
set.seed(12345)
params_tuned_glmnet = tuneParams(lrn_glmnet, task = task, resampling = inner,
                                 par.set = ps_glmnet, control = ctrl, 
                                 measure = list(rmse))
parallelStop()
```

#### Tuned results

We use the `s` value from the tuning. 
As we will always use the complete dataset, this should be the best value.

```{r}
lrn_glmnet_tuned <- makeLearner("regr.glmnet",
                                alpha = 0,
                                s = 1810,
                                standardize = FALSE, 
                                intercept = FALSE)
#lambda = sort(seq(0, 5, length.out = 100), decreasing = T))
glmnet_train_tuned = mlr::train(lrn_glmnet_tuned, task)
```

Inspect the highest absolute coefficients (positive and negative).
The first entry is the intercept and is therefore left out.

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810)))) %>% 
  arrange(desc(coef))
```
```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810)))) %>% 
  arrange(coef)
```

```{r}
plot(glmnet_train_tuned$learner.model, "lambda")
```

```{r}
plot(coef(glmnet_train_tuned$learner.model))
```

