---
title: "03-modeling"
output: 
  bookdown::html_document2:
    toc: true
    toc_float: true
---

# Load packages

```{r, results='hide'}
#devtools::install_github("tidyverse/ggplot2")
devtools::install_github("mlr-org/mlr@blocking")

library(mlr)
library(mlrMBO)
library(glue)
library(sf)
library(dplyr)
library(tibble)
library(purrr)
library(magrittr)
library(data.table)
library(pbmcapply)
```

# Read data

Variables per plot:

Lauki1: 7594
Laukiz2: 7594
Oiartzun: 7841
Luiando: 7471

Why is there such a huge difference in the number of indices without NAs for the plots?

```{r}
data = readRDS("/data/patrick/mod/hyperspectral/data_clean_with_indices/data_clean_standardized_bf2.rda")
coords = readRDS("/data/patrick/mod/hyperspectral/data_clean_with_indices/data_clean_standardized_bf2-coords.rda")
```

# Merge into one dataset

```{r}
data = as_tibble(rbindlist(data, fill = TRUE))
data = Filter(function(x) !any(is.na(x)), data)
coords = as_tibble(rbindlist(coords))
```

Save the indices shared by all plots so that we can subset the single plots to use the same indices

```{r}
indices_shared = names(data)
saveRDS(indices_shared, "/data/patrick/mod/hyperspectral/data_clean_with_indices/indices-shared-by-all-plots-bf2.rda")
```

# Create task

```{r}
task = makeRegrTask(id = "all_plots", data = data, 
                    target = "defoliation", coordinates = coords,
                    blocking = factor(rep(1:4, c(479, 451, 291, 529))))
```

# Modeling

## glmnet

### Ridge

We only need to tune `s`: https://github.com/mlr-org/mlr/issues/1030#issuecomment-233677172
`glmnet` fits many models during training. However, important is which `s` is chosen for prediction. 
The value of `s` determines which trained model will be chosen.
If `s` != `lambda`, linear interpolation or refitting (this) is applied.

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda` values.

Besides tuning with `mlr`, we could use `cv-glmnet()` to find a suitable `lambda` value for a given dataset.
However, we would do a random partitioning instead of a spatial partitioning here.

Coefficients of ridge regression should not be used for inference as they are biased. 
They can be seen as a variable importance measure. 
The highest coefficients are most important for the model (if all variables are on the same scale) https://stats.stackexchange.com/a/154712/101464

```{r}
# predict.type for 'auc' (AUROC)
lrn_glmnet <- makeLearner("regr.glmnet",
                          alpha = 0,
                          standardize = FALSE,
                          intercept = FALSE)
                          #lambda = sort(seq(0, 5, length.out = 100), decreasing = T))
# getHyperPars(lrn_glmnet)
# getParamSet(lrn_glmnet)
```

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda`.

```{r}
glmnet_train = mlr::train(lrn_glmnet, task)
summary(glmnet_train$learner.model$lambda)
```

```{r}
ps_glmnet <- makeParamSet(makeNumericParam("s", lower = 35, upper = 3559))
```

```{r}
ctrl = makeMBOControl(propose.points = 1L)
ctrl = setMBOControlTermination(ctrl, iters = 70L)
ctrl = setMBOControlInfill(ctrl, crit = crit.ei)
ctrl = setMBOControlMultiPoint(ctrl, method = "cl", cl.lie = min)
tune.ctrl = makeTuneControlMBO(mbo.control = ctrl,
                               mbo.design = generateDesign(n = 30, par.set =  ps_glmnet))
inner <- makeResampleDesc("Blocking")
```

#### SpCV

```{r}
tune_wrapper <- makeTuneWrapper(lrn_glmnet, resampling = inner, par.set = ps_glmnet,
                                control = tune.ctrl, show.info = TRUE,
                                measures = list(rmse))
```

```{r}
# outer <- makeResampleDesc("RepCV", folds = 4, reps = 2)
outer <- makeResampleDesc("Blocking")
```

```{r}
library(parallelMap)
parallelStart(mode = "multicore", level = "mlrMBO.feval", cpus = 40,
              mc.set.seed = TRUE) # only parallelize the tuning

set.seed(12345)
resa_glmnet <- mlr::resample(tune_wrapper, task,
                             resampling = outer, extract = getTuneResult,
                             show.info = TRUE, measures = list(rmse))
parallelStop()
saveRDS(resa_glmnet, "/data/patrick/mod/hyperspectral/spcv/all_plots_ridge_bf2_10folds_blocking.rda")
```

Calculate repetition mean

```{r}
resa_glmnet = readRDS("/data/patrick/mod/hyperspectral/spcv/all_plots_ridge_bf2_10folds_blocking.rda")
resa_glmnet$measures.test %>% 
  summarise(mean(rmse), sd(rmse))
```

#### Visualize partitions

```{r}
resa_glmnet= readRDS("/data/patrick/mod/hyperspectral/spcv/all_plots_ridge_bf2_10folds.rda")
plot = createSpatialResamplingPlots(task, list("Blocking" = resa_glmnet), crs = 32630)
cowplot::plot_grid(plotlist = plot[["Plots"]], ncol = 4, nrow = 1, labels = plot[["Labels"]])
```

#### TuneParams

```{r}
configureMlr(on.learner.error = "warn", on.error.dump = TRUE)
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.tuneParams", cpus = 48,
              mc.set.seed = TRUE) # only parallelize the tuning
set.seed(12345)
params_tuned_glmnet = tuneParams(lrn_glmnet, task = task, resampling = inner,
                                 par.set = ps_glmnet, control = tune.ctrl, 
                                 show.info = TRUE, measure = list(rmse))
parallelStop()
saveRDS(params_tuned_glmnet, "/data/patrick/mod/tmp/tuning_hyperspectral/tuning_bf2_ridge_blocking.rda")
```

#### Tuned results

We use the `s` value from the tuning. 
As we will always use the complete dataset, this should be the best value.

```{r}
lrn_glmnet_tuned <- makeLearner("regr.glmnet",
                                alpha = 0,
                                s = 875,
                                standardize = FALSE, 
                                intercept = FALSE)

glmnet_train_tuned = mlr::train(lrn_glmnet_tuned, task)
```

Inspect the highest absolute coefficients (positive and negative).
The first entry is the intercept and is therefore left out.

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 875))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 875)))) %>% 
  arrange(desc(coef))
```

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 875))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 875)))) %>% 
  arrange(coef)
```

```{r}
plot(glmnet_train_tuned$learner.model, "lambda")
```

```{r}
plot(coef(glmnet_train_tuned$learner.model))
```

#### Important spectral regions

Get the first 100 most important indices.
We take the absolute value of the coefficient to consider both sides.

We only take the top 100 indices so that the result is not cluttered by mid-ranged indices summping up in the end-result.

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 875))), 
       coef = abs(as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 875))))) %>% 
  arrange(desc(coef)) %>% 
    mutate(coef = coef) -> top_inds_rr_all_plots
saveRDS(top_inds_rr_all_plots, "/data/patrick/mod/hyperspectral/top-inds/top_inds_rr_all_plots.rda")
```

### Lasso

We only need to tune `s`: https://github.com/mlr-org/mlr/issues/1030#issuecomment-233677172
`glmnet` fits many models during training. However, important is which `s` is chosen for prediction. 
The value of `s` determines which trained model will be chosen.
If `s` != `lambda`, linear interpolation or refitting is applied.

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda` values.

Besides tuning with `mlr`, we could use `cv-glmnet()` to find a suitable `lambda` value for a given dataset.
However, we would do a random partitioning instead of a spatial partitioning here.

Coefficients of ridge regression should not be used for inference as they are biased. 
They can be seen as a variable importance measure then? The highest coefficients are most important for the model (if all variables are on the same scale) https://stats.stackexchange.com/a/154712/101464

```{r}
lrn_glmnet <- makeLearner("regr.glmnet",
                          alpha = 1,
                          standardize = FALSE,
                          intercept = FALSE)
# getHyperPars(lrn_glmnet)
# getParamSet(lrn_glmnet)
```

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda`.

```{r}
glmnet_train = mlr::train(lrn_glmnet, task)
summary(glmnet_train$learner.model$lambda)
```

```{r}
ps_glmnet <- makeParamSet(makeNumericParam("s", lower = 0.03, upper = 3.5))
```

```{r}
ctrl = makeMBOControl(propose.points = 1L)
ctrl = setMBOControlTermination(ctrl, iters = 70L)
ctrl = setMBOControlInfill(ctrl, crit = crit.ei)
ctrl = setMBOControlMultiPoint(ctrl, method = "cl", cl.lie = min)
tune.ctrl = makeTuneControlMBO(mbo.control = ctrl,
                               mbo.design = generateDesign(n = 30, par.set =  ps_glmnet))
inner <- makeResampleDesc("SpCV", iters = 10)
inner <- makeResampleDesc("Blocking")
```

#### SpCV

```{r}
tune_wrapper <- makeTuneWrapper(lrn_glmnet, resampling = inner, par.set = ps_glmnet,
                                control = tune.ctrl, show.info = TRUE,
                                measures = list(rmse))
```

```{r}
outer <- makeResampleDesc("Blocking")
```

```{r}
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.resample", cpus = 48,
              mc.set.seed = TRUE) # only parallelize the tuning

set.seed(12345)
resa_glmnet <- mlr::resample(tune_wrapper, task,
                             resampling = outer, extract = getTuneResult,
                             show.info = TRUE, measures = list(rmse))

parallelStop()
saveRDS(resa_glmnet, "/data/patrick/mod/hyperspectral/spcv/all_plots_lasso_bf2_10folds_blocking.rda")
```

Calculate repetition mean

```{r}
resa_glmnet = readRDS("/data/patrick/mod/hyperspectral/spcv/all_plots_lasso_bf2_10folds_blocking.rda")
resa_glmnet$measures.test %>% 
  summarise(mean(rmse), sd(rmse))
```

#### TuneParams

```{r}
configureMlr(on.learner.error = "warn", on.error.dump = TRUE)
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.tuneParams", cpus = 40,
              mc.set.seed = TRUE) # only parallelize the tuning
set.seed(12345)
params_tuned_glmnet = tuneParams(lrn_glmnet, task = task, resampling = inner,
                                 par.set = ps_glmnet, control = tune.ctrl, 
                                 measure = list(rmse))
parallelStop()
```

#### Tuned results

We use the `s` value from the tuning. 
As we will always use the complete dataset, this should be the best value.

```{r}
lrn_glmnet_tuned <- makeLearner("regr.glmnet",
                                alpha = 0,
                                s = 1810,
                                standardize = FALSE, 
                                intercept = FALSE)
#lambda = sort(seq(0, 5, length.out = 100), decreasing = T))
glmnet_train_tuned = mlr::train(lrn_glmnet_tuned, task)
```

Inspect the highest absolute coefficients (positive and negative).
The first entry is the intercept and is therefore left out.

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810)))) %>% 
  arrange(desc(coef))
```

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810)))) %>% 
  arrange(coef)
```

```{r}
plot(glmnet_train_tuned$learner.model, "norm")
```

```{r}
plot(coef(glmnet_train_tuned$learner.model))
```

### Elasticnet

We only need to tune `s`: https://github.com/mlr-org/mlr/issues/1030#issuecomment-233677172
`glmnet` fits many models during training. However, important is which `s` is chosen for prediction. 
The value of `s` determines which trained model will be chosen.
If `s` != `lambda`, linear interpolation or refitting is applied.

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda` values.

Besides tuning with `mlr`, we could use `cv-glmnet()` to find a suitable `lambda` value for a given dataset.
However, we would do a random partitioning instead of a spatial partitioning here.

Coefficients of ridge regression should not be used for inference as they are biased. 
They can be seen as a variable importance measure then? The highest coefficients are most important for the model (if all variables are on the same scale) https://stats.stackexchange.com/a/154712/101464

```{r}
lrn_glmnet <- makeLearner("regr.glmnet",
                          standardize = FALSE,
                          intercept = FALSE)
#getHyperPars(lrn_glmnet)
#getParamSet(lrn_glmnet)
```

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda`.

```{r}
glmnet_train = mlr::train(lrn_glmnet, task)
summary(glmnet_train$learner.model$lambda)
```

For the elasticnet we also tune `alpha`.
As we have both penalties, we just use the lower and uppper maxima from both Lasso and Ridge.

```{r}
ps_glmnet <- makeParamSet(makeNumericParam("s", lower = 0.03, upper = 3559),
                          makeNumericParam("alpha", lower = 0, upper = 1))
```

```{r}
ctrl = makeMBOControl(propose.points = 1L)
ctrl = setMBOControlTermination(ctrl, iters = 70L)
ctrl = setMBOControlInfill(ctrl, crit = crit.ei)
ctrl = setMBOControlMultiPoint(ctrl, method = "cl", cl.lie = min)
tune.ctrl = makeTuneControlMBO(mbo.control = ctrl,
                               mbo.design = generateDesign(n = 30, par.set = ps_glmnet))
inner <- makeResampleDesc("Blocking")
```

#### SpCV

```{r}
tune_wrapper <- makeTuneWrapper(lrn_glmnet, resampling = inner, par.set = ps_glmnet,
                                control = tune.ctrl, show.info = TRUE,
                                measures = list(rmse))
```

```{r}
outer <- makeResampleDesc("Blocking")
```

```{r}
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.resample", cpus = 40,
              mc.set.seed = TRUE) # only parallelize the tuning

set.seed(12345)
resa_glmnet <- mlr::resample(tune_wrapper, task,
                             resampling = outer, extract = getTuneResult,
                             show.info = TRUE, measures = list(rmse))

parallelStop()
saveRDS(resa_glmnet, "/data/patrick/mod/hyperspectral/spcv/all_plots_elasticnet_blocking.rda")
```

#### TuneParams

```{r}
configureMlr(on.learner.error = "warn", on.error.dump = TRUE)
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.tuneParams", cpus = 48,
              mc.set.seed = TRUE) # only parallelize the tuning
set.seed(12345)
params_tuned_glmnet = tuneParams(lrn_glmnet, task = task, resampling = inner,
                                 par.set = ps_glmnet, control = tune.ctrl, 
                                 measure = list(rmse))
parallelStop()
```

#### Tuned results

We use the `s` value from the tuning. 
As we will always use the complete dataset, this should be the best value.

```{r}
lrn_glmnet_tuned <- makeLearner("regr.glmnet",
                                alpha = 0,
                                s = 1810,
                                standardize = FALSE, 
                                intercept = FALSE)
#lambda = sort(seq(0, 5, length.out = 100), decreasing = T))
glmnet_train_tuned = mlr::train(lrn_glmnet_tuned, task)
```

Inspect the highest absolute coefficients (positive and negative).
The first entry is the intercept and is therefore left out.

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810)))) %>% 
  arrange(desc(coef))
```
```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810)))) %>% 
  arrange(coef)
```

```{r}
plot(glmnet_train_tuned$learner.model, "lambda")
```

```{r}
plot(coef(glmnet_train_tuned$learner.model))
```

### SVM

```{r}
lrn_ksvm <- makeLearner("regr.ksvm",
                        kernel = "rbfdot")
```

```{r}
ps_svm <- makeParamSet(makeNumericParam("C", lower = -15, upper = 15,
                                        trafo = function(x) 2 ^ x),
                       makeNumericParam("sigma", lower = -15, upper = 15,
                                        trafo = function(x) 2 ^ x))
```

```{r}
ctrl = makeMBOControl(propose.points = 1L)
ctrl = setMBOControlTermination(ctrl, iters = 70L)
ctrl = setMBOControlInfill(ctrl, crit = crit.ei)
ctrl = setMBOControlMultiPoint(ctrl, method = "cl", cl.lie = min)
tune.ctrl = makeTuneControlMBO(mbo.control = ctrl,
                               mbo.design = generateDesign(n = 30, par.set = ps_svm))
inner <- makeResampleDesc("Blocking")
```

#### SpCV

```{r}
tune_wrapper <- makeTuneWrapper(lrn_ksvm, resampling = inner, par.set = ps_svm,
                                control = tune.ctrl, show.info = TRUE,
                                measures = list(rmse))
```

```{r}
outer <- makeResampleDesc("Blocking")
```

```{r}
library(parallelMap)
parallelStart(mode = "multicore", cpus = 40, level = "mlrMBO.feval",
              mc.set.seed = TRUE) # only parallelize the tuning
set.seed(12345)
resa_svm <- mlr::resample(tune_wrapper, task,
                          resampling = outer, extract = getTuneResult,
                          show.info = TRUE, measures = list(rmse))

parallelStop()
saveRDS(resa_svm, "/data/patrick/mod/hyperspectral/spcv/all_plots_svm_blocking.rda")
```

### xgboost

```{r}
lrn_xgboost <- makeLearner("regr.xgboost",
                   par.vals = list(
                     objective = "reg:linear",
                     eval_metric = "error",
                     eta = 0.1
                   ))
```

```{r}
ps_xgboost = makeParamSet(
  makeDiscreteParam("nrounds", values = c(10, 100, 1000, 10000)),
  makeNumericParam("colsample_bytree", lower=0.1, upper=1),
  makeNumericParam("subsample", lower=0.1, upper=1),
  makeIntegerParam("max_depth", lower=1, upper=785),
  makeNumericParam("gamma", lower=0, upper=20)
)
```

```{r}
ctrl = makeMBOControl(propose.points = 1L)
ctrl = setMBOControlTermination(ctrl, iters = 70L)
ctrl = setMBOControlInfill(ctrl, crit = crit.ei)
ctrl = setMBOControlMultiPoint(ctrl, method = "cl", cl.lie = min)
tune.ctrl = makeTuneControlMBO(mbo.control = ctrl,
                               mbo.design = generateDesign(n = 30, par.set = ps_xgboost))
inner <- makeResampleDesc("Blocking")
```

#### SpCV

```{r}
tune_wrapper <- makeTuneWrapper(lrn_xgboost, resampling = inner, par.set = ps_xgboost,
                                control = tune.ctrl, show.info = TRUE,
                                measures = list(rmse))
```

```{r}
outer <- makeResampleDesc("Blocking")
```

```{r}
library(parallelMap)
parallelStart(mode = "multicore", cpus = 40, level = "mlrMBO.feval",
              mc.set.seed = TRUE) # only parallelize the tuning
set.seed(12345)
resa_svm <- mlr::resample(tune_wrapper, task,
                          resampling = outer, extract = getTuneResult,
                          show.info = TRUE, measures = list(rmse))

parallelStop()
saveRDS(resa_svm, "/data/patrick/mod/hyperspectral/spcv/all_plots_xgboost_blocking.rda")
```
