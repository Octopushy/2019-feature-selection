---
title: "03-modeling"
output: 
  bookdown::html_document2:
    toc: true
    toc_float: true
---

# Load packages

```{r, results='hide'}
library(mlr)
library(glue)
library(sf)
library(dplyr)
library(tibble)
library(purrr)
library(magrittr)
library(data.table)
library(pbmcapply)
```

# Read data

Variables per plot:

Lauki1: 7594
Laukiz2: 7594
Oiartzun: 7891
Luiando: 6073

Why is there such a huge difference in the number of indices without NAs for the plots?

```{r}
data = readRDS("/data/patrick/mod/hyperspectral/data_clean_with_indices/data_clean_standardized_bf1.rda")
coords = readRDS("/data/patrick/mod/hyperspectral/data_clean_with_indices/data_clean_standardized_bf1-coords.rda")
```

# Merge into one dataset

Leaving out Luiando (see README for reason).

```{r}
data = as_tibble(rbindlist(data, fill = TRUE))
data = Filter(function(x) !any(is.na(x)), data)
coords = as_tibble(rbindlist(coords))
```

# Create task

```{r}
task = makeRegrTask(id = "laukiz1", data = data, 
                    target = "deftot", coordinates = coords)
```

# Modeling

## glmnet

### Ridge

We only need to tune `s`: https://github.com/mlr-org/mlr/issues/1030#issuecomment-233677172
`glmnet` fits many models during training. However, important is which `s` is chosen for prediction. 
The value of `s` determines which trained model will be chosen.
If `s` != `lambda`, linear interpolation or refitting is applied.

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda` values.

Besides tuning with `mlr`, we could use `cv-glmnet()` to find a suitable `lambda` value for a given dataset.
However, we would do a random partitioning instead of a spatial partitioning here.

Coefficients of ridge regression should not be used for inference as they are biased. 
They can be seen as a variable importance measure then? The highest coefficients are most important for the model (if all variables are on the same scale) https://stats.stackexchange.com/a/154712/101464

```{r}
# predict.type for 'auc' (AUROC)
lrn_glmnet <- makeLearner("regr.glmnet",
                          alpha = 0,
                          standardize = FALSE)
                          #lambda = sort(seq(0, 5, length.out = 100), decreasing = T))
getHyperPars(lrn_glmnet)
getParamSet(lrn_glmnet)
```

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda`.

```{r}
glmnet_train = mlr::train(lrn_glmnet, task)
summary(glmnet_train$learner.model$lambda)
```

```{r}
ps_glmnet <- makeParamSet(makeNumericParam("s", lower = 30, upper = 3300))
```

```{r}
ctrl <- makeTuneControlRandom(maxit = 500)
inner <- makeResampleDesc("SpCV", iters = 5)
```

#### TuneParams

```{r}
configureMlr(on.learner.error = "warn", on.error.dump = TRUE)
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.tuneParams", cpus = 40,
              mc.set.seed = TRUE) # only parallelize the tuning
set.seed(12345)
params_tuned_glmnet = tuneParams(lrn_glmnet, task = task, resampling = inner,
                                 par.set = ps_glmnet, control = ctrl, 
                                 measure = list(rmse))
parallelStop()
saveRDS("/data/patrick/mod/tmp/tuning_hyperspectral/tuning_bf1_ridge.rda")
```

#### SpCV

```{r}
tune_wrapper <- makeTuneWrapper(lrn_glmnet, resampling = inner, par.set = ps_glmnet,
                                control = ctrl, show.info = TRUE,
                                measures = list(rmse))
```

```{r}
outer <- makeResampleDesc("SpRepCV", folds = 10, reps = 2)
```

```{r}
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.tuneParams", cpus = 40,
              mc.set.seed = TRUE) # only parallelize the tuning

set.seed(12345)
resa_glmnet <- mlr::resample(tune_wrapper, task,
                             resampling = outer, extract = getTuneResult,
                             show.info = TRUE, measures = list(rmse))

parallelStop()
```

#### Tuned results

We use the `s` value from the tuning. 
As we will always use the complete dataset, this should be the best value.

```{r}
lrn_glmnet_tuned <- makeLearner("regr.glmnet",
                                alpha = 0,
                                s = 1810,
                                standardize = FALSE, 
                                intercept = FALSE)
#lambda = sort(seq(0, 5, length.out = 100), decreasing = T))
glmnet_train_tuned = mlr::train(lrn_glmnet_tuned, task)
```

Inspect the highest absolute coefficients (positive and negative).
The first entry is the intercept and is therefore left out.

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810)))) %>% 
  arrange(desc(coef))
```
```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810)))) %>% 
  arrange(coef)
```

```{r}
plot(glmnet_train_tuned$learner.model, "lambda")
```

```{r}
plot(coef(glmnet_train_tuned$learner.model))
```

#### Variable importance

```{r}
library(parallelMap)
parallelStart(mode = "multicore", cpus = 40,
              mc.set.seed = TRUE) # only parallelize the tuning
set.seed(12345)
feature_imp = generateFeatureImportanceData(task, 
                                            method = "permutation.importance",
                                            lrn_glmnet_tuned,
                                            measure = list(rmse),
                                            nmc = 40)
parallelStop()

saveRDS(feature_imp, "/data/patrick/mod/hyperspectral/variable-importance/bf1-nmc40.rda")


arrange(feature_imp$res)

melt(feature_imp$res) %>% 
  arrange(desc(value))
```

### Lasso

We only need to tune `s`: https://github.com/mlr-org/mlr/issues/1030#issuecomment-233677172
`glmnet` fits many models during training. However, important is which `s` is chosen for prediction. 
The value of `s` determines which trained model will be chosen.
If `s` != `lambda`, linear interpolation or refitting is applied.

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda` values.

Besides tuning with `mlr`, we could use `cv-glmnet()` to find a suitable `lambda` value for a given dataset.
However, we would do a random partitioning instead of a spatial partitioning here.

Coefficients of ridge regression should not be used for inference as they are biased. 
They can be seen as a variable importance measure then? The highest coefficients are most important for the model (if all variables are on the same scale) https://stats.stackexchange.com/a/154712/101464

```{r}
# predict.type for 'auc' (AUROC)
lrn_glmnet <- makeLearner("regr.glmnet",
                          alpha = 1,
                          standardize = FALSE)
                          #lambda = sort(seq(0, 5, length.out = 100), decreasing = T))
getHyperPars(lrn_glmnet)
getParamSet(lrn_glmnet)
```

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda`.

```{r}
glmnet_train = mlr::train(lrn_glmnet, task)
summary(glmnet_train$learner.model$lambda)
```

```{r}
ps_glmnet <- makeParamSet(makeNumericParam("s", lower = 0.03, upper = 3.3))
```

```{r}
ctrl <- makeTuneControlRandom(maxit = 500)
inner <- makeResampleDesc("SpCV", iters = 5)
```

#### TuneParams

```{r}
configureMlr(on.learner.error = "warn", on.error.dump = TRUE)
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.tuneParams", cpus = 40,
              mc.set.seed = TRUE) # only parallelize the tuning
set.seed(12345)
params_tuned_glmnet = tuneParams(lrn_glmnet, task = task, resampling = inner,
                                 par.set = ps_glmnet, control = ctrl, 
                                 measure = list(rmse))
parallelStop()
```



#### SpCV

```{r}
tune_wrapper <- makeTuneWrapper(lrn_glmnet, resampling = inner, par.set = ps_glmnet,
                                control = ctrl, show.info = TRUE,
                                measures = list(rmse))
```

```{r}
outer <- makeResampleDesc("SpRepCV", folds = 10, reps = 2)
```

```{r}
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.tuneParams", cpus = 40,
              mc.set.seed = TRUE) # only parallelize the tuning

set.seed(12345)
resa_glmnet <- mlr::resample(tune_wrapper, task,
                             resampling = outer, extract = getTuneResult,
                             show.info = TRUE, measures = list(rmse))

parallelStop()
saveRDS("/data/patrick/mod/tmp/tuning_hyperspectral/tuning_bf1_lasso.rda")
```

#### Tuned results

We use the `s` value from the tuning. 
As we will always use the complete dataset, this should be the best value.

```{r}
lrn_glmnet_tuned <- makeLearner("regr.glmnet",
                                alpha = 0,
                                s = 1810,
                                standardize = FALSE, 
                                intercept = FALSE)
#lambda = sort(seq(0, 5, length.out = 100), decreasing = T))
glmnet_train_tuned = mlr::train(lrn_glmnet_tuned, task)
```

Inspect the highest absolute coefficients (positive and negative).
The first entry is the intercept and is therefore left out.

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810)))) %>% 
  arrange(desc(coef))
```
```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 1810)))) %>% 
  arrange(coef)
```

```{r}
plot(glmnet_train_tuned$learner.model, "lambda")
```

```{r}
plot(coef(glmnet_train_tuned$learner.model))
```

#### Variable importance

```{r}
library(parallelMap)
parallelStart(mode = "multicore", cpus = 40,
              mc.set.seed = TRUE) # only parallelize the tuning
set.seed(12345)
feature_imp = generateFeatureImportanceData(task, 
                                            method = "permutation.importance",
                                            lrn_glmnet_tuned,
                                            measure = list(rmse),
                                            nmc = 40)
parallelStop()

saveRDS(feature_imp, "/data/patrick/mod/hyperspectral/variable-importance/bf1-nmc40.rda")


arrange(feature_imp$res)

melt(feature_imp$res) %>% 
  arrange(desc(value))
```
