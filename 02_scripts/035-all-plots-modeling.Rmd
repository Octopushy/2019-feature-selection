---
title: "03-modeling"
output: 
  bookdown::html_document2:
    toc: true
    toc_float: true
---

# Load packages

```{r, results='hide'}
devtools::install_github("mlr-org/mlr@blocking")

library(mlr)
library(mlrMBO)
library(iml)
library(glue)
library(sf)
library(dplyr)
library(tibble)
library(purrr)
library(magrittr)
library(data.table)
library(pbmcapply)
library(ggpubr)
```

# Read data

Variables per plot:

Lauki1: 7594
Laukiz2: 7594
Oiartzun: 7841
Luiando: 7471

Why is there such a huge difference in the number of indices without NAs for the plots?

```{r}
data = readRDS("/data/patrick/mod/hyperspectral/data_clean_with_indices/data_clean_standardized_bf2.rda")
coords = readRDS("/data/patrick/mod/hyperspectral/data_clean_with_indices/data_clean_standardized_bf2-coords.rda")
```

# Merge into one dataset

```{r}
data = as_tibble(rbindlist(data, fill = TRUE))
data = Filter(function(x) !any(is.na(x)), data)
coords = as_tibble(rbindlist(coords))
```

Save the indices shared by all plots so that we can subset the single plots to use the same indices.

```{r}
indices_shared = names(data)
saveRDS(indices_shared, "/data/patrick/mod/hyperspectral/data_clean_with_indices/indices-shared-by-all-plots-bf2.rda")
```

# Create task

```{r}
task = makeRegrTask(id = "all_plots", data = data, 
                    target = "defoliation", coordinates = coords,
                    blocking = factor(rep(1:4, c(479, 451, 291, 529))))
```

# Modeling

## Ridge

We only need to tune `s`: https://github.com/mlr-org/mlr/issues/1030#issuecomment-233677172
`glmnet` fits many models during training. However, important is which `s` is chosen for prediction. 
The value of `s` determines which trained model will be chosen.
If `s` != `lambda`, linear interpolation or refitting (this) is applied.

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda` values.

Besides tuning with `mlr`, we could use `cv-glmnet()` to find a suitable `lambda` value for a given dataset.
However, we would do a random partitioning instead of a spatial partitioning here.

Coefficients of ridge regression should not be used for inference as they are biased. 
They can be seen as a variable importance measure. 
The highest coefficients are most important for the model (if all variables are on the same scale) https://stats.stackexchange.com/a/154712/101464

```{r}
lrn_glmnet <- makeLearner("regr.glmnet",
                          alpha = 0,
                          standardize = FALSE,
                          intercept = FALSE)
```

To find a suitable tuning space for `s`, Julia recommends to do a simple `train()` call and check the range of the chosen `lambda`.

```{r}
glmnet_train = mlr::train(lrn_glmnet, task)
summary(glmnet_train$learner.model$lambda)
```

```{r}
ps_glmnet <- makeParamSet(makeNumericParam("s", lower = 35, upper = 3559))
```

```{r}
ctrl = makeMBOControl(propose.points = 1L)
ctrl = setMBOControlTermination(ctrl, iters = 70L)
ctrl = setMBOControlInfill(ctrl, crit = crit.ei)
tune.ctrl = makeTuneControlMBO(mbo.control = ctrl,
                               mbo.design = generateDesign(n = 30, 
                                                           par.set =  ps_glmnet))
```

```{r}
inner <- makeResampleDesc("Blocking")
outer <- makeResampleDesc("Blocking")
```

#### SpCV

```{r}
tune_wrapper <- makeTuneWrapper(lrn_glmnet, resampling = inner, par.set = ps_glmnet,
                                control = tune.ctrl, show.info = TRUE,
                                measures = list(rmse))
```

```{r}
library(parallelMap)
parallelStart(mode = "multicore", level = "mlrMBO.feval", cpus = 40,
              mc.set.seed = TRUE) 

set.seed(12345)
resa_glmnet <- mlr::resample(tune_wrapper, task,
                             resampling = outer, extract = getTuneResult,
                             show.info = TRUE, measures = list(rmse))
parallelStop()
saveRDS(resa_glmnet, "/data/patrick/mod/hyperspectral/spcv/all_plots_ridge_bf2_10folds_blocking.rda")
```

Calculate repetition mean

```{r}
resa_glmnet = readRDS("/data/patrick/mod/hyperspectral/spcv/all_plots_ridge_bf2_10folds_blocking.rda")
resa_glmnet$measures.test %>% 
  summarise(mean(rmse), sd(rmse))
```

#### Visualize partitions

```{r}
resa_glmnet= readRDS("/data/patrick/mod/hyperspectral/spcv/all_plots_ridge_bf2_10folds.rda")
plot = createSpatialResamplingPlots(task, list("Blocking" = resa_glmnet), crs = 32630)
cowplot::plot_grid(plotlist = plot[["Plots"]], ncol = 4, nrow = 1, labels = plot[["Labels"]])
```

#### TuneParams

```{r}
configureMlr(on.learner.error = "warn", on.error.dump = TRUE)
library(parallelMap)
parallelStart(mode = "multicore", level = "mlr.tuneParams", cpus = 48,
              mc.set.seed = TRUE) 
set.seed(12345)
params_tuned_glmnet = tuneParams(lrn_glmnet, task = task, resampling = inner,
                                 par.set = ps_glmnet, control = tune.ctrl, 
                                 show.info = TRUE, measure = list(rmse))
parallelStop()
saveRDS(params_tuned_glmnet, "/data/patrick/mod/tmp/tuning_hyperspectral/tuning_bf2_ridge_blocking.rda")
```

#### Tuned results

We use the `s` value from the tuning. 
As we will always use the complete dataset, this should be the best value.

```{r}
lrn_glmnet_tuned <- makeLearner("regr.glmnet",
                                alpha = 0,
                                s = 875,
                                standardize = FALSE, 
                                intercept = FALSE)

glmnet_train_tuned = mlr::train(lrn_glmnet_tuned, task)
```

Inspect the highest absolute coefficients (positive and negative).
The first entry is the intercept and is therefore left out.

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 875))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 875)))) %>% 
  arrange(desc(coef))
```

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 875))), 
       coef = as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 875)))) %>% 
  arrange(coef)
```

```{r}
plot(glmnet_train_tuned$learner.model, "lambda")
```

```{r}
plot(coef(glmnet_train_tuned$learner.model))
```

#### Important spectral regions

Get the first 100 most important indices.
We take the absolute value of the coefficient to consider both sides.

We only take the top 100 indices so that the result is not cluttered by mid-ranged indices summping up in the end-result.

```{r}
tibble(index = rownames(as.matrix(coef(glmnet_train_tuned$learner.model, s = 875))), 
       coef = abs(as.vector(as.matrix(coef(glmnet_train_tuned$learner.model, s = 875))))) %>% 
  arrange(desc(coef)) %>% 
    mutate(coef = coef) -> top_inds_rr_all_plots
saveRDS(top_inds_rr_all_plots, "/data/patrick/mod/hyperspectral/top-inds/top_inds_rr_all_plots.rda")
```

## SVM

```{r}
lrn_ksvm <- makeLearner("regr.ksvm",
                        kernel = "rbfdot")
```

```{r}
ps_svm <- makeParamSet(makeNumericParam("C", lower = -15, upper = 15,
                                        trafo = function(x) 2 ^ x),
                       makeNumericParam("sigma", lower = -15, upper = 15,
                                        trafo = function(x) 2 ^ x))
```

```{r}
ctrl = makeMBOControl(propose.points = 1L)
ctrl = setMBOControlTermination(ctrl, iters = 70L)
ctrl = setMBOControlInfill(ctrl, crit = crit.ei)
ctrl = setMBOControlMultiPoint(ctrl, method = "cl", cl.lie = min)
tune.ctrl = makeTuneControlMBO(mbo.control = ctrl,
                               mbo.design = generateDesign(n = 30, par.set = ps_svm))
inner <- makeResampleDesc("Blocking")
```

#### SpCV

```{r}
tune_wrapper <- makeTuneWrapper(lrn_ksvm, resampling = inner, par.set = ps_svm,
                                control = tune.ctrl, show.info = TRUE,
                                measures = list(rmse))
```

```{r}
outer <- makeResampleDesc("Blocking")
```

```{r}
library(parallelMap)
parallelStart(mode = "multicore", cpus = 40, level = "mlrMBO.feval",
              mc.set.seed = TRUE) # only parallelize the tuning
set.seed(12345)
resa_svm <- mlr::resample(tune_wrapper, task,
                          resampling = outer, extract = getTuneResult,
                          show.info = TRUE, measures = list(rmse))

parallelStop()
saveRDS(resa_svm, "/data/patrick/mod/hyperspectral/spcv/all_plots_svm_blocking.rda")
```

## xgboost

```{r}
lrn_xgboost <- makeLearner("regr.xgboost",
                   par.vals = list(
                     objective = "reg:linear",
                     eval_metric = "error"
                   ))
```

```{r}
ps_xgboost <- makeParamSet(
  makeIntegerParam("nrounds", lower = 10, upper = 600),
  makeNumericParam("colsample_bytree", lower = 0.3, upper = 0.7),
  makeNumericParam("subsample", lower = 0.25, upper = 1),
  makeIntegerParam("max_depth", lower = 1, upper = 10),
  makeNumericParam("gamma", lower = 0, upper = 10),
  makeNumericParam("eta", lower = 0.001, upper = 0.6),
  makeNumericParam("min_child_weight", lower = 0, upper = 20)
)
```

```{r}
ctrl = makeMBOControl(propose.points = 1L)
ctrl = setMBOControlTermination(ctrl, iters = 20L)
ctrl = setMBOControlInfill(ctrl, crit = crit.ei)
tune.ctrl = makeTuneControlMBO(mbo.control = ctrl,
                               mbo.design = generateDesign(n = 30, 
                                                           par.set = ps_xgboost))
```

```{r}
inner <- makeResampleDesc("Blocking")
outer <- makeResampleDesc("Blocking")
```

#### SpCV

```{r}
tune_wrapper <- makeTuneWrapper(lrn_xgboost, resampling = inner, par.set = ps_xgboost,
                                control = tune.ctrl, show.info = TRUE,
                                measures = list(rmse))
```

If we parallelize the tuning using `level = "mlrMBO.feval"` for some unknown reason everything stalls after the first outer fold.
That's why the parallelization takes place on the resample level.

```{r}
library(parallelMap)
parallelStart(mode = "multicore", cpus = 4, level = "mlr.resample",
              mc.set.seed = TRUE)
set.seed(12345)
resa_svm <- mlr::resample(tune_wrapper, task,
                          resampling = outer, extract = getTuneResult,
                          show.info = TRUE, measures = list(rmse))
parallelStop()
saveRDS(resa_svm, "/data/patrick/mod/hyperspectral/spcv/all_plots_xgboost_blocking.rda")
```

#### Tuning

```{r}
configureMlr(on.learner.error = "warn", on.error.dump = TRUE)
library(parallelMap)
parallelStart(mode = "multicore", level = "mlrMBO.feval", cpus = 48,
              mc.set.seed = TRUE)
set.seed(12345)
xgboost_tuned = tuneParams(lrn_xgboost, task = task, resampling = inner,
                           par.set = ps_xgboost, control = tune.ctrl, 
                           show.info = TRUE, measure = list(rmse))
parallelStop()
saveRDS(xgboost_tuned, "/data/patrick/mod/tmp/tuning_hyperspectral/xgboost_tuned_blocking.rda")
```

#### Train

```{r}
xgboost_tuned = readRDS("/data/patrick/mod/tmp/tuning_hyperspectral/xgboost_tuned_blocking.rda")

lrn_xgboost = setHyperPars(makeLearner("regr.xgboost"), 
                           par.vals = xgboost_tuned$x)
m = train(lrn_xgboost, task)
saveRDS(m, "/data/patrick/mod/hyperspectral/prediction/xgboost_trained_tuned.rda")
```

### Variable importance

#### mlr

We can either use the Feat Imp from `xgboost` by calling `getFeatureImportance()` directly on the fitted model or calculate it again using permutation via `generateFeatureImportanceData()`.

```{r}
# 5h 30 min
var_imp = generateFeatureImportanceData(task, learner = lrn_xgboost, 
                                        measure = rmse, nmc = 2)
saveRDS(var_imp, "/data/patrick/mod/hyperspectral/variable-importance/bf2_ncm2.rda")


var_imp$res = tidyr::gather(var_imp$res, "index", "feat.imp")
var_imp$res %>% 
  arrange(desc(feat.imp)) %>% 
  slice(1:30) %>% 
  ggdotchart(x = "index", y = "feat.imp",                              # Color by groups
             sorting = "asc",                       # Sort value in descending order
             add = "segments",                             # Add segments from y = 0 to dots
             #color = "index",
             palette = "nejm",
             #position = position_dodge(1),
             group = "index",
             #title = .y,
             rotate = T,                                # Rotate vertically
             dot.size = 0.5,                                 # Large dot size
             ggtheme = theme_pubr(base_size = 6, margin = F,
                                  legend = "none")) + 
  labs(y = "Score", color = "Spectral region")
```

Visualize top 30 indices

```{r}
imp.xgb = getFeatureImportance(m) 
imp.xgb$res = tidyr::gather(imp.xgb$res, "index", "feat.imp")

imp.xgb$res %>% 
  arrange(desc(feat.imp)) %>% 
  slice(1:30) %>% 
  ggdotchart(x = "index", y = "feat.imp",                              # Color by groups
             sorting = "asc",                       # Sort value in descending order
             add = "segments",                             # Add segments from y = 0 to dots
             #color = "index",
             palette = "nejm",
             #position = position_dodge(1),
             group = "index",
             #title = .y,
             rotate = T,                                # Rotate vertically
             dot.size = 0.5,                                 # Large dot size
             ggtheme = theme_pubr(base_size = 6, margin = F,
                                  legend = "none")) + 
  labs(y = "Score", color = "Spectral region")
```
```{r}
ggsave(here::here("03_figures/results/var-imp-xgboost.png"), height = 3,
       width = 5)
```

#### iml

```{r}
X = task$env$data[which(names(task$env$data) != "defoliation")]
predictor = Predictor$new(m, data = X, y = task$env$data$defoliation)
imp = FeatureImp$new(predictor, loss = "rmse")
```

Partial influence

```{r}
X = task$env$data[which(names(task$env$data) != "defoliation")]
predictor = Predictor$new(m, data = X, y = task$env$data$defoliation)
pdp.obj = Partial$new(predictor, feature = "bf2_EVI")
pdp.obj$plot()
```

## RF

```{r}
# RF
ps_rf <- makeParamSet(
  makeIntegerParam("mtry", lower = 1, upper = 3000),
  makeIntegerParam("min.node.size", lower = 1, upper = 10),
  makeNumericParam("sample.fraction", lower = 0.2, upper = 0.9)
)
ctrl <- makeMBOControl(propose.points = 1L)
ctrl <- setMBOControlTermination(ctrl, iters = 20L)
ctrl <- setMBOControlInfill(ctrl, crit = crit.ei)
ctrl <- setMBOControlMultiPoint(ctrl, method = "cl", cl.lie = min)
tune.ctrl <- makeTuneControlMBO(
  mbo.control = ctrl,
  mbo.design = generateDesign(n = 30, par.set = ps_rf)
)
inner <- makeResampleDesc("Blocking")

tune_wrapper_rf <- makeTuneWrapper(makeLearner("regr.ranger"),
                                   resampling = inner, par.set = ps_rf,
                                   control = tune.ctrl, show.info = TRUE,
                                   measures = list(rmse)
)
```

```{r}
outer <- makeResampleDesc("Blocking")
```

```{r}
library(parallelMap)
parallelStart(mode = "multicore", cpus = 40, level = "mlrMBO.feval",
              mc.set.seed = TRUE) # only parallelize the tuning
set.seed(12345)
resa_svm <- mlr::resample(tune_wrapper_rf, task,
                          resampling = outer, extract = getTuneResult,
                          show.info = TRUE, measures = list(rmse))

parallelStop()
saveRDS(resa_svm, "/data/patrick/mod/hyperspectral/spcv/all_plots_rf_blocking.rda")
```
